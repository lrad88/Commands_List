cloud computing : on demand delivery of IT resources and apps through the internet with PAYG pricing

the 3 cloud computing deplyoment models: cloud based, on premises and hybrid

- cloud based: run all parts of the app on the cloud, existing appps get put on the cloud, new apps get put on cloud

- on premises: aka private cloud deployment, applications that run on a data center on premises but incoporate application management and virtualization technologies which increase resource utilization

- hybrid: cloud based resources connected to on premises infrastructure, and legacy IT applications. A good use case is to replicate on site system architecture in the cloud for testing before deployment on local infrastructure.

benefits of cloud computing:

upfront infrastructure expense vs variable expenses that can be adjusted based on your companies needs. capacity of your IT infrastructure no longer needs to be guessed and projected making your company more agile and limiting wastage of resources. this agility allows for more flexibility to experiment and innovate. low latency

=================================================================================

EC2 - amazon elastic compute cloud - gives you access to amazon server resources, and control of server configuration, operates on windows or linux, runs this virtual machine on top of hypervisor where you can install your apps and databases. ec2 allows you to change how much ram, HDD space, graphics, and cpu processing is needed at the click of a button, thisis also where you configure your network traffic. Multiple ec2 instances can be launched at once and represent a vm with control of hardware resources. generally at the t2.micro instance it would cost 0.0116 USD per hour on general Linux, different OS greatly fluctuate costs

EC2 instance types -  Instance types consist of a prefix identifying the type of workloads they’re optimized for or instance family, followed by a size. For example, the instance type c5n.xlarge can be broken down as follows:

First position – The first position, c, indicates the instance family. This indicates that this instance belongs to the compute optimized family.

Second position – The second position, 5, indicates the generation of the instance. This instance belongs to the fifth generation of instances.

Remaining letters before the period – In this case, n indicates additional attributes, such as local NVMe storage.

After the period – After the period, xlarge indicates the instance size. In this example, it's xlarge.

amazon EC2 instance families: general purpose, compute optimized, memory optimized, accelerated computing, storage optimized, high performance compute optimized. these are system hardware configurations.

AMI - When launching an EC2 instance you first must configure the operating system you want using an Amazon Machine Image. When you launch a new instance, AWS allocates a virtual machine that runs on a hypervisor. Then the AMI that you selected is copied to the root device volume, which contains the image that is used to boot the volume. In the end, you get a server that you can connect to and install packages and additional software on. AMIs are reusable presets for OS, programs and configurations on the VM and can be copied to multiple EC2 vms. AMIS can be found in quickstart AMIs in the AWS marketplace, AWSc communities and your own custom AMIs. 

Each AMI in the AWS Management Console has an AMI ID, which is prefixed by ami-, followed by a random hash of numbers and letters. The IDs are unique to each AWS Region.

Multitenancy: sharing underlying hardware between virtual machines
caas: compute as a service

EC2 purchase options: on demand by far the most expensive but the most flexible. Reserved instances  offers 1 or 3 year contracts when you commit to an instance family, tenancy and region, reserved instance is a billing discount applied to on demand that limits its capability , spot instance are used for workloads with flexible start and end times and are ok with interruptions like batch processing offers the highest discounts. dedicated hosts are servers fully dedicated to your use only and are the most expensive

EC2 when do you get charged - When launching an instane it enters a pending state which you are not charged for. When it starts running is when you get charged. When rebooting an instance is the same as rebooting your os he instance keeps its public DNS name (IPv4) and private and public IPv4 addresses. An IPv6 address (if applicable) remains on the same host computer and maintains its public and private IP address, in addition to any data on its instance store volumes. Lastly when you stop your instance or are stopping you are still being charged, only works when you have an EBS physical volume as the root, retains IP addresses. same with stop hibernate which saves the state for faster boot next time.

costs stop being incurred when an instance is shutdown or terminated

Terminating an instance: this will erase the instance nd you will lose all config including Ip addresses.

ec2 locations- unless speccified, when you launch ec2 instances they are in a default vpc. the
default vpc is public and accessible by the internet so dont place any customer data or private information. this should replaced with a custom vpc when you know how to configure it

amazon ec2 autoscaling - allows for server resources to auto scale based on your companies needs, based on dynamic or predictive scaling or both

vertical scaling - adding more power to machines that are running. Increase the instance size. If too many requests are sent to a single active-passive system, the active server will become unavailable and, hopefully, fail over to the passive server. But this doesn’t solve anything.

With active-passive systems, you need vertical scaling. This means increasing the size of the server. With EC2 instances, you select either a larger type or a different instance type. This can be done only while the instance is in a stopped state. In this scenario, the following steps occur:

Stop the passive instance. This doesn’t impact the application because it’s not taking any traffic.
    Change the instance size or type, and then start the instance again.
    Shift the traffic to the passive instance, turning it active.
    Stop, change the size, and start the previous active instance because both instances should match.

When the number of requests reduces, you must do the same operation. Even though there aren’t that many steps involved, it’s actually a lot of manual work. Another disadvantage is that a server can only scale vertically up to a certain limit. When that limit is reached, the only option is to create another active-passive system and split the requests and functionalities across them. This can require massive application rewriting.

This is where the active-active system can help. When there are too many requests, you can scale this system horizontally by adding more servers.

horizontal scaling - adding more instances, Add additional instances. As mentioned, for the application to work in an active-active system, it’s already created as stateless, not storing any client sessions on the server. This means that having two or four servers wouldn’t require any application changes. It would only be a matter of creating more instances when required and shutting them down when traffic decreases. The Amazon EC2 Auto Scaling service can take care of that task by automatically creating and removing EC2 instances based on metrics from Amazon CloudWatch. We will learn more about this service in this lesson.
You can see that there are many more advantages to using an active-active system in comparison with an active-passive system. Modifying your application to become stateless provides scalability.

auto scaling components: There are three main components of Amazon EC2 Auto Scaling. Each of these components addresses one main question as follows:

Launch template or configuration: Which resources should be automatically scaled?
Amazon EC2 Auto Scaling groups: Where should the resources be deployed?
Scaling policies: When should the resources be added or removed?
you can set the minimum, desired or maximum number of instances for your autoscaling group

templates: You can use a launch template to manually launch an EC2 instance or for use with Amazon EC2 Auto Scaling. It also supports versioning, which can be used for quickly rolling back if there's an issue or a need to specify a default version of the template. This way, while iterating on a new version, other users can continue launching EC2 instances using the default version until you make the necessary changes. You can create a launch template in one of three ways as follows:
Use an existing EC2 instance. All the settings are already defined.

Create one from an already existing template or a previous version of a launch template.

Create a template from scratch. These parameters will need to be defined: AMI ID, instance type, key pair, security group, storage, and resource tags.

autoscaling groups:

The next component Amazon EC2 Auto Scaling needs is an Amazon EC2 Auto Scaling group. An Auto Scaling group helps you define where Amazon EC2 Auto Scaling deploys your resources. This is where you specify the Amazon Virtual Private Cloud (Amazon VPC) and subnets the EC2 instance should be launched in. Amazon EC2 Auto Scaling takes care of creating the EC2 instances across the subnets, so select at least two subnets that are across different Availability Zones.

With Auto Scaling groups, you can specify the type of purchase for the EC2 instances. You can use On-Demand Instances or Spot Instances. You can also use a combination of the two, which means you can take advantage of Spot Instances with minimal administrative overhead.

To specify how many instances Amazon EC2 Auto Scaling should launch, you have three capacity settings to configure for the group size.

minimum capacity: This is the minimum number of instances running in your Auto Scaling group, even if the threshold for lowering the number of instances is reached.
When Amazon EC2 Auto Scaling removes EC2 instances because the traffic is minimal, it keeps removing EC2 instances until it reaches a minimum capacity. 
When reaching that limit, even if Amazon EC2 Auto Scaling is instructed to remove an instance, it does not. This ensures that the minimum is kept. Note: Depending on your application, using a minimum of two is recommended to ensure high availability. However, you ultimately know how many EC2 instances at a bare minimum your application requires at all times.

desired capacity: The desired capacity is the number of EC2 instances that Amazon EC2 Auto Scaling creates at the time the group is created. This number can only be within or equal to the minimum or maximum. 
If that number decreases, Amazon EC2 Auto Scaling removes the oldest instance by default. If that number increases, Amazon EC2 Auto Scaling creates new instances using the launch template.

maximum capacity: This is the maximum number of instances running in your Auto Scaling group, even if the threshold for adding new instances is reached.
When traffic keeps growing, Amazon EC2 Auto Scaling keeps adding EC2 instances. This means the cost for your application will also keep growing. That’s why you must set a maximum amount to ensure it doesn’t go above your budget.

scaling policies - By default, an Auto Scaling group will be kept to its initial desired capacity. While it’s possible to manually change the desired capacity, you can also use scaling policies.

simple scaling policy: With a simple scaling policy, you can do exactly what’s described in this module. You use a CloudWatch alarm and specify what to do when it is invoked. This can include adding or removing a number of EC2 instances or specifying a number of instances to set the desired capacity to. You can specify a percentage of the group instead of using a number of EC2 instances, which makes the group grow or shrink more quickly.
After the scaling policy is invoked, it enters a cooldown period before taking any other action. This is important because it takes time for the EC2 instances to start, and the CloudWatch alarm might still be invoked while the EC2 instance is booting. For example, you might decide to add an EC2 instance if the CPU utilization across all instances is above 65 percent. You don’t want to add more instances until that new EC2 instance is accepting traffic. However, what if the CPU utilization is now above 85 percent across the Auto Scaling group?
Adding one instance might not be the right move. Instead, you might want to add another step in your scaling policy. Unfortunately, a simple scaling policy can’t help with that. This is where a step scaling policy helps. 

step scaling policy: Step scaling policies respond to additional alarms even when a scaling activity or health check replacement is in progress. Similar to the previous example, you might decide to add two more instances when CPU utilization is at 85 percent and four more instances when it’s at 95 percent.
Deciding when to add and remove instances based on CloudWatch alarms might seem like a difficult task. This is why the third type of scaling policy exists—target tracking.

target tracking scaling policy: If your application scales based on average CPU utilization, average network utilization (in or out), or request count, then this scaling policy type is the one to use. All you need to provide is the target value to track, and it automatically creates the required CloudWatch alarms.

elastic load balancing elb - directs traffic to servers in an even way so that each servers load is balanced, runs at the region level, automatically scalable, a single url that the entire front end uses to communicate with the backend, makes backend frontend communication much more simple. automatically distributes incoming traffic across multiple targets in one or more availability zones. Load balancing refers to the process of distributing tasks across a set of resources. In the case of the Employee Directory application, the resources are EC2 instances that host the application, and the tasks are the requests being sent. You can use a load balancer to distribute the requests across all the servers hosting the application.
To do this, the load balancer needs to take all the traffic and redirect it to the backend servers based on an algorithm. The most popular algorithm is round robin, which sends the traffic to each server one after the other. The ELB service provides a major advantage over using your own solution to do load balancing. Mainly, you don’t need to manage or operate ELB. It can distribute incoming application traffic across EC2 instances, containers, IP addresses, and Lambda functions. Other key features include the following:

elb with ec2 autoscaling: s soon as a new EC2 instance is added to or removed from the Amazon EC2 Auto Scaling group, ELB is notified. However, before ELB can send traffic to a new EC2 instance, it needs to validate that the application running on the EC2 instance is available.
This validation is done by way of the ELB health checks feature you learned about in the previous lesson. features include integration with amazon ec2 autoscaling directring incoming traffic to instances

Hybrid mode – Because ELB can load balance to IP addresses, it can work in a hybrid mode, which means it also load balances to on-premises servers.
High availability – ELB is highly available. The only option you must ensure is that the load balancer's targets are deployed across multiple Availability Zones.
Scalability – In terms of scalability, ELB automatically scales to meet the demand of the incoming traffic. It handles the incoming traffic and sends it to your backend application.

health checks - Monitoring is an important part of load balancers because they should route traffic to only healthy EC2 instances. That’s why ELB supports two types of health checks as follows:

Establishing a connection to a backend EC2 instance using TCP and marking the instance as available if the connection is successful.
Making an HTTP or HTTPS request to a webpage that you specify and validating that an HTTP response code is returned.

Taking time to define an appropriate health check is critical. Only verifying that the port of an application is open doesn’t mean that the application is working. It also doesn’t mean that making a call to the home page of an application is the right way either.

After determining the availability of a new EC2 instance, the load balancer starts sending traffic to it. If ELB determines that an EC2 instance is no longer working, it stops sending traffic to it and informs Amazon EC2 Auto Scaling. It is the responsibility of Amazon EC2 Auto Scaling to remove that instance from the group and replace it with a new EC2 instance. Traffic is only sent to the new instance if it passes the health check.
If Amazon EC2 Auto Scaling has a scaling policy that calls for a scale down action, it informs ELB that the EC2 instance will be terminated. ELB can prevent Amazon EC2 Auto Scaling from terminating an EC2 instance until all connections to the instance end. It also prevents any new connections. This feature is called connection draining. We will learn more about Amazon EC2 Auto Scaling in the next lesson.

elb components - three componentes = rule: To associate a target group to a listener, you must use a rule. Rules are made up of two conditions. The first condition is the source IP address of the client. The second condition decides which target group to send the traffic to. 

Listener: The client connects to the listener. This is often called client side. To define a listener, a port must be provided in addition to the protocol, depending on the load balancer type. There can be many listeners for a single load balancer.

target group:  The backend servers, or server side, are defined in one or more target groups. This is where you define the type of backend you want to direct traffic to, such as EC2 instances, Lambda functions, or IP addresses. Also, a health check must be defined for each target group. 

load balancer types- 

application load balancer -  for User authorization, Rich metrics and logging, Redirects, Fixed response. An Application Load Balancer functions at Layer 7 of the Open Systems Interconnection (OSI) model. It is ideal for load balancing HTTP and HTTPS traffic. After the load balancer receives a request, it evaluates the listener rules in priority order to determine which rule to apply. It then routes traffic to targets based on the request content. The primary features of alb include:

routes traffic based on request data: An Application Load Balancer makes routing decisions based on the HTTP and HTTPS protocol. For example, the ALB could use the URL path (/upload) and host, HTTP headers and method, or the source IP address of the client. This facilitates granular routing to target groups.

sends responses directly to the client: An Application Load Balancer can reply directly to the client with a fixed response, such as a custom HTML page. It can also send a redirect to the client. This is useful when you must redirect to a specific website or redirect a request from HTTP to HTTPS. It removes that work from your backend servers.

uses tls offloading: An Application Load Balancer understands HTTPS traffic. To pass HTTPS traffic through an Application Load Balancer, an SSL certificate is provided one of the following ways:

    Importing a certificate by way of IAM or ACM services
    Creating a certificate for free using ACM

This ensures that the traffic between the client and Application Load Balancer is encrypted.

authenticates users: An Application Load Balancer can authenticate users before they can pass through the load balancer. The Application Load Balancer uses the OpenID Connect (OIDC) protocol and integrates with other AWS services to support protocols, such as the following:

    SAML
    Lightweight Directory Access Protocol (LDAP)
    Microsoft Active Directory
    Others

secures traffic: To prevent traffic from reaching the load balancer, you configure a security group to specify the supported IP address ranges.

supports sticky sessions:  If requests must be sent to the same backend server because the application is stateful, use the sticky session feature. This feature uses an HTTP cookie to remember which server to send the traffic to across connections.

network load balancer -  TCP and User Datagram Protocol (UDP) connection based, Source IP preservation, Low latency. A Network Load Balancer is ideal for load balancing TCP and UDP traffic. It functions at Layer 4 of the OSI model, routing connections from a target in the target group based on IP protocol data, works with tls . primary features of nlb's include:

sticky sessions: Routes requests from the same client to the same target.
low latency: Offers low latency for latency-sensitive applications.
source ip address: Preserves the client-side source IP address.
static ip support: Automatically provides a static IP address per Availability Zone (subnet).
elastic ip address support: Lets users assign a custom, fixed IP address per Availability Zone (subnet).
dns failover: Uses Amazon Route 53 to direct traffic to load balancer nodes in other zones.

gateway load balancer: works with ip traffic, Health checks, Gateway Load Balancer Endpoints, Higher availability for third-party virtual appliances. A Gateway Load Balancer helps you to deploy, scale, and manage your third-party appliances, such as firewalls, intrusion detection and prevention systems, and deep packet inspection systems. It provides a gateway for distributing traffic across multiple virtual appliances while scaling them up and down based on demand. here are som feratures of the glb:

high availability: Ensures high availability and reliability by routing traffic through healthy virtual appliances.
monitoring: Can be monitored using CloudWatch metrics.
streamlined deployments: Can deploy a new virtual appliance by selecting it in the AWS Marketplace.
private connectivity: Connects internet gateways, virtual private clouds (VPCs), and other network resources over a private network.

messaging and queing
=================================================================================
  
tightly coupled vs loosely coupled architecture or microservices model - when an architecture is coupled tightly the applications within it rely heavily upon each other to operate and if one goes down so does the other, loosely coupled is the opposite and components within it are more isolated, a buffer or message qeue is added between the 2 applications which lists all incoming requests and stores them if they are unable to be processed instead of losing them, there are 2 different messaging services:

simple notification service: uses a publish and subscribe model, and SNS topics (messaging channels) where users can subscribe to a topic and recieve messages

simple qeue service: send store and recieve messages between software components at any volume, stored in a system called sqs queues, allows decoupling of system components and buffering between processes

tightly coupled architrcture is not possible on aws
=================================================================================
serverless compute - you cannot see or access the underying infrastructure or instances that are hosting your app, all management of the system is taken care of for you, serverless compute options include 

aws lambda - is a serverless compute service  where you upload your code to a lambda function which is configured to have a trigger to begin the code. designed to run code under 15 minutes so its for quick processing like web back end or handling requests, web apps and websites, internet of things, data processing eg. batch, real time, map reduce, ML inference, chatbots, amazon alexa or voice enabled applcations, IT automation and infrastruture management. Lambda function can be configured using Lambda api, cloud formation, aws serverless application model SAM.

An event is a JSON-formatted document that contains data for a Lambda function to process. The runtime converts the event to an object and passes it to your function code. When you invoke a function, you determine the structure and contents of the event.i

With Lambda, you can run code without provisioning or managing servers, and you pay only for what you use. You are charged for the number of times that your code is invoked (requests) and for the time that your code runs, rounded up to the nearest 1 millisecond (ms) of duration.
AWS rounds up duration to the nearest ms with no minimum run time. With this pricing, it can be cost effective to run functions whose execution time is very low, such as functions with durations under 100 ms or low latency APIs.

aws fargate - serverless compute platform for ecs and eks, back end is managed for you, It achieves this by allocating the right amount of compute. This eliminates the need to choose and manage EC2 instances, cluster capacity, and scaling. 

container - a  docker container, all containers share the same OS kernel,  a package for your code its dependencies and configuration, containers run on top of ec2 instances and run in isolation. containers havea shorter boot up time then VMs which allows for super fast respone to ultra high demand. solves the issue of getting wsoftware to run erliably when moved from one compute environment to another.A container is a standardized unit that packages your code and its dependencies. This package is designed to run reliably on any platform, because the container creates its own independent environment. With containers, workloads can be carried from one place to another, such as from development to production or from on-premises environments to the cloud.

Containers share the same operating system and kernel as the host that they exist on. But virtual machines contain their own operating system. Each virtual machine must maintain a copy of an operating system, which results in a degree of wasted resources. A container is more lightweight. Containers spin up quicker, almost instantly. This difference in startup time becomes instrumental when designing applications that must scale quickly during I/O bursts.
Containers can provide speed, but virtual machines offer the full strength of an operating system and more resources, like package installation, dedicated kernel, and more.


docker - uses os level virtualization to deliver software in containers, a software platform that allows you to build, test and deploy applications quickly. a popular container runtime that simplifies the management of the entire operating system stack required for container isolation, including networking and storage. Docker helps customers create, package, deploy, and run containers.

kubernetes - enables you to deploy and manage containeized applications at a large scale, many conatainers, many clusters 

container orchestration - processes to start stop and monitor containers over multiple ec2 instances. containrers can run on ec2 instances. If you’re trying to manage your compute at a large scale, you should consider the following:
How to place your containers on your instances, What happens if your container fails, What happens if your instance fails, How to monitor deployments of your containers. This coordination is handled by a container orchestration service

amazon elastic container service, amazon elastic kubernetes service - these services are container orchestration tools and are run without the use of of conatiner orchestration software, can be run on top of ec2. helps you spin up new containers. requires install of ecs container agent on your ec2 instance called a container instance. With ECS you can do the following: Launching and stopping containers, Getting cluster state, Scaling in and out, Scheduling the placement of containers across your cluster, Assigning permissions, Meeting availability requirements. To prepare your application to run on Amazon ECS, you create a task definition. The task definition is a text file, in JSON format, that describes one or more containers. A task definition is similar to a blueprint that describes the resources that you need to run a container, such as CPU, memory, ports, images, storage, and networking information. Here is a simple task definition that you can use for your corporate directory application. In this example, this runs on the Nginx web server.

{
"family": "webserver",
"containerDefinitions": [ {
"name": "web",
"image": "nginx",
"memory": "100",
"cpu": "99"
} ],
"requiresCompatibilities": [ "FARGATE" ],
"networkMode": "awsvpc",
"memory": "512",
"cpu": "256"
}

Amazon EKS is a managed service that you can use to run Kubernetes on AWS without needing to install, operate, and maintain your own Kubernetes control plane or nodes. Amazon EKS is conceptually similar to Amazon ECS, but with the following differences:In Amazon ECS, the machine that runs the containers is an EC2 instance that has an ECS agent installed and configured to run and manage your containers. This instance is called a container instance. In Amazon EKS, the machine that runs the containers is called a worker node or Kubernetes node. An ECS container is called a task. An EKS container is called a pod.

Amazon ECS runs on AWS native technology. Amazon EKS runs on Kubernetes. 

Suppose that a company’s application developer has an environment on their computer that is different from the environment on the computers used by the IT operations staff. The developer wants to ensure that the application’s environment remains consistent regardless of deployment, so they use a containerized approach. This helps to reduce time spent debugging applications and diagnosing differences in computing environments. 

AWS global insfratructure:
=================================================================================

aws regions - eg. us-east, aws builds data centers all over the world close to places of high demand all interconnected on a high speed fiber network, customers get to choose which region they run their operation from and it can either explicitly be kept in that one region or be exported to another with right security credentials, this is important for security of information where certain data isnt allowed to leave a country. if youre not asked to specify an individual availability zone upon deployment this indicates that the service operates on a region-scoped level, in this case aws automatically performs actions to increase data durability and availability on the other hand if you are asked to specify an availiability zone then these tasks fall to you, this involves making sure your workload is replicated across multiple availability zones, you should use 2 availability zones at a minimum which will at least make your data redundant incase an availability zone fails, 4 main factors to consider when selecting a region are latency, price, service availability and compliance  

4 factors are involved in selecting region: 

compliance with the law which requires you to handle your data in a certain way for example sensitive data cannot leave the boundaries of the united kingdom, proximity or latency is an important consideration if you are  building high speed applications like stock monitoring, feature availability (different regions have different features AWS services available to them), pricing (different regions equal different pricing)

availability zone: eg. us-east-1a, the "1a or 1b" is the availability zone. data centers are located in availability zones which are located in regions, brazil is a region for example.  fully isolated portion of the aws infrastructure, one single region can contain many data centers or availability zones which helps with disaster proofing and threat management, most applications run across at least 2 availability zones

edge locations - or edge caches run amazon cloud front, and amazon route 53 which a dynamic name service or dns. used to cache constent closer to end users, thus reducing latency 

amazon cloud front - a content delivery network or cdn that uses edge locations which are separate from regions. regions can send data to edge locations to make it geographically closer for people to access reducing trasnfer times of data

aws outposts - extend aws infrstructure and services to different locations can install a fully operational mini region right inside your own data center isolated within your own building


interacting with AWS
=================================================================================

API - Every action you make in AWS is an API call thats authenticated and authorized. application programming interface are used to interact with AWS services either through AWS management console, AWS command line interface, AWS software development kits, AWS cloud formation to create requests to send to aws apis to create and manage aws resources 

AWS management console - browser based system typically used by noobs, easy to understand and increase knowledge of aws services, useful for building test environments, aws billing, monitoring resources, slow way of doing things because you need to manually design each instance with a browser.

AWS CLI - allows you to script api calls instead of manually doing it on a browser, you can program them to run on a trigger or at a certain time and it limits errors from manual manipulation. can be accessed either by downloading the CLI to your computer or through the AWS console.
For example, you run the following API call against a service, using the AWS CLI:
aws s3api list-buckets
You will get a response similar to the following one, listing the buckets in your AWS accounts:

{
    "Owner": {
        "DisplayName": "tech-essentials", 
        "ID": "d9881f40b83adh2896eb276f44ffch53677faec805422c83dfk60cc335a7da92"
    }, 
    "Buckets": [
        {
            "CreationDate": "2023-01-10T15:50:20.000Z", 
            "Name": "aws-tech-essentials"
        }, 
        {
            "CreationDate": "2023-01-10T16:04:15.000Z", 
            "Name": "aws-tech-essentials-employee-directory-app"
        } 
    ]
}


AWS software development kits SDK - interact with aws resources through various programming languages eg. python , nodejs, ruby etc. Useful when you want to integrate your application source code with AWS services. For example, consider an application with a frontend that runs in Python. Every time the application receives a photo, it uploads the file to a storage service. This action can be achieved in the source code by using the AWS SDK for Python (Boto3). Here is an example of code that you can implement to work with AWS resources using the SDK for Python.

import boto3
ec2 = boto3.client('ec2')
response = ec2.describe_instances()
print(response)

aws elastic beanstalk: a service thats takes your application code and configurations and builds your regional environment for you, this setup can be saved on beanstalk and redeployed with ease. focuses on application over infrastrucutre

aws cloud formation: an infrastructure as code tool using JSOn or yaml text based documents called cloud formation documents. allows you to specify what infrastructure you wanto build without specifying how it will be built


Networking introduction:
=================================================================================

ip address basics - 

a basic ip address is 32 bits of 1s and 0s in binary

IPV4 notation: converts these 32 bits into 4 groups of 8 bits, each one of these 8 bits creates a unique number to create a unique ip address eg. 192.168.1.42

CIDR notation: 192.168.1.30 is a single IP address. If you want to express IP addresses between the range of 192.168.1.0 and 192.168.1.255, how can you do that? One way is to use CIDR notation. CIDR notation is a compressed way of representing a range of IP addresses. Specifying a range determines how many IP addresses are available to you. eg. 192.168.1.0/24 It begins with a starting IP address and is separated by a forward slash (the / character) followed by a number. The number at the end specifies how many of the bits of the IP address are fixed. In this example, the first 24 bits of the IP address are fixed. The rest (the last 8 bits) are flexible ot the last octet or the last number after the decimal point, the 0.

When working with networks in the AWS Cloud, you choose your network size by using CIDR notation. In AWS, the smallest IP range you can have is /28, which provides 16 IP addresses. The largest IP range you can have is a /16, which provides 65,536 IP addresses.

these numbered steps  need to be done to setup your VPC -----

1. Amazon virtual private cloud : the main way networks are configured in aws. launch aws resources in a virtual network with access to the internet or private aka subnets. vpc controls ip addresses and subnets, different resources are placed in different subnets. eg. ec2 instances or elb's. you need 2 pieces of info to setup a vpc: the region and the CIDR IP address range.

2. subnets: are chunks of ip addresses within your vpc that allow you to group resources together and controls whether resources are publically or privately available. Think of subnets as smaller networks inside your base network or virtual LANS in a traditional network.  to create a subnet you need a vpc, availability zone, CIDR range. use public subnets for the internet and private subnets that wont be connected to the internet

3. internet gateway: needs to be created and then attached to your vpc to gain access to the internet, literally connects you to the raw internet

4. virtual private gateway: connects your vpc to another private network. When you create and attach a virtual private gateway to a VPC, the gateway acts as anchor on the AWS side of the connection. On the other side of the connection, you will need to connect a customer gateway to the other private network. A customer gateway device is a physical device or software application on your side of the connection. When you have both gateways, you can then establish an encrypted virtual private network (VPN) connection between the two sides.

5. create a redundant network to reduce downtime and have high availability on a separate availability zone to increase redundancy. say you have one az with ip address 10.1.0.0/16, public subnet 10.1.1.0/24 and private subnet 10.1.3.0/24 you should have a redundant network with the same ip address along with a public subnet of 10.1.2.0/24 and private subnet of 10.1.4.0/24 

6. create route tables to direct network traffic, A main route table is made automatically by default and will direct traffic to all subnets that dont have an explicit route table built for them, this is not correct ifyou have private subnets so 2 separate route tables need to be made to direct traffic to the public and private subnets as you want, to make a public route table to connect to an internet gateway you enter edit routes and add a destination usually 0.0.0.0/0 with an internet gateway target and then associate the public subnets, the same needs to be done for your private route table but associate it with your private subnets and dont connect it to the internet, a local target will suffice.  The following rules apply to the main route table:

    You cannot delete the main route table.
    You cannot set a gateway route table as the main route table.
    You can replace the main route table with a custom subnet route table.
    You can add, remove, and modify routes in the main route table.
    You can explicitly associate a subnet with the main route table, even if it's already implicitly associated. 

7. secure your subnets with network access control lists, these are virtual firewalls for your subnets that control what kind of traffic is allowed to enter or leave your subnet, you can configure this with inbound and outbound rules that can prevent HTTPS traffic for example. Network ACLs are considered stateless, so you need to include both the inbound and outbound ports used for the protocol. If you don’t include the outbound range, your server would respond but the traffic would never leave the subnet.

8. Secure EC2 instanes with security groups, another virtual firewall for EC2 instances. the default configuration of a security group blocks all inbound traffic and allows all outbound traffic. By default, a security group only allows outbound traffic. To allow inbound traffic, you must create inbound rules.You might be wondering, “Wouldn’t this block all EC2 instances from receiving the response of any customer requests?” Well, security groups are stateful. That means that they will remember if a connection is originally initiated by the EC2 instance or from the outside, and temporarily allow traffic to respond without modifying the inbound rules. If you want your EC2 instance to accept traffic from the internet, you must open up inbound ports. If you have a web server, you might need to accept HTTP and HTTPS requests to allow that type of traffic into your security group. You learned earlier that subnets can be used to segregate traffic between computers in your network. Security groups can be used in the same way. A common design pattern is to organize resources into different groups and create security groups for each to control network communication between them. This example defines three tiers and isolates each tier with defined security group rules. In this case, internet traffic to the web tier is allowed over HTTPS. Web tier to application tier traffic is allowed over HTTP, and application tier to database tier traffic is allowed over MySQL. This is different from traditional on-premises environments, in which you isolate groups of resources with a VLAN configuration. In AWS, security groups allow you to achieve the same isolation without tying the security groups to your network.

reserved IPs - For AWS to configure your VPC appropriately, AWS reserves five IP addresses in each subnet. These IP addresses are used for routing, Domain Name System (DNS), and network management. For example, consider a VPC with the IP range 10.0.0.0/22. The VPC includes 1,024 total IP addresses. This is then divided into four equal-sized subnets, each with a /24 IP range with 256 IP addresses. Out of each of those IP ranges, there are only 251 IP addresses that can be used because AWS reserves five.
AWS reserves five IP addresses in each subnet that cannot be assigned to a resource.

The five reserved IP addresses can impact how you design your network. A common starting place for those who are new to the cloud is to create a VPC with an IP range of /16 and create subnets with an IP range of /24. This provides a large amount of IP addresses to work with at both the VPC and subnet levels.

aws direct connect - a completely private dedicated fiber connection from your data center to aws. much more reliable, secure and expensive compared to virtual private gateway or vpn, or public internet gateway. To establish a secure physical connection between your on-premises data center and your Amazon VPC, you can use AWS Direct Connect. With AWS Direct Connect, your internal network is linked to an AWS Direct Connect location over a standard Ethernet fiber-optic cable. This connection allows you to create virtual interfaces directly to public AWS services or to your VPC.

NACL - network access control list is a virtual firewall used by a subnet to control what packets are given access to the subnet from the gateway. by default NACLs  allow all  any packets in and out

 stateless packet filtering - means everytime a packet is passed in and out of a subnet it is checked on a list of allowed and disallowed items. by default all packets are allowed in and out without checks = stateless

security group - another virtual firewall that controls admission into ec2 instances after they have passed the subnet layer, control is done so by specifying the type of traffic the instance will accept eg. https, os, admin requests, after the traffic is allowed in it will automatically be allowed back out without a security check.

stateful packet control allows all return traffic to be allowed back out of the security group and into a previous security group without checks, it remembers packets that have been allowed in and allows them back out with no checks

global networking: how does customer access your aws infrastructure

route 53 : aws domain name service. routes  domain names to ip addresses based on latency, geolocation, geoproximity and weighted round robin to give the customer the least latent data. route 53 is also used to register and buy domain names, connects user requests to infrastructure in AWS and outside AWS 

amazon cloud front: is a content delivery network and connects edge networks to primary infrastructure to get the least latent data to the customer


Storage:
=================================================================================

ec2 gives access to all physical resources including storage, lets look at storage services in aws.

AWS storage services are grouped into 3 categories: File storage, block storage and object storage.  In file storage, data is stored as files in a hierarchy. In block storage, data is stored in fixed-size blocks. And in object storage, data is stored as objects in buckets

file storage: used in windows file explorer or MAC finder. files are organized in a tree like heirarchy that consist of folders and subfolders. Each file has metadata such as file name, file size, and the date the file was created. The file also has a path, for example, computer/Application_files/Cat_photos/cats-03.png. When you need to retrieve a file, your system can use the path to find it in the file hierarchy. File storage is ideal when you require centralized access to files that must be easily shared and managed by multiple host computers. Typically, this storage is mounted onto multiple hosts, and requires file locking and integration with existing file system communication protocols. Use cases for file storage:

web serving: Cloud file storage solutions follow common file-level protocols, file naming conventions, and permissions that developers are familiar with. Therefore, file storage can be integrated into web applications.

Analytics: Many analytics workloads interact with data through a file interface and rely on features such as file lock or writing to portions of a file. Cloud-based file storage supports common file-level protocols and has the ability to scale capacity and performance. Therefore, file storage can be conveniently integrated into analytics workflows.

Media and entrtainment: Many businesses use a hybrid cloud deployment and need standardized access using file system protocols (NFS or SMB) or concurrent protocol access. Cloud file storage follows existing file system semantics. Therefore, storage of rich media content for processing and collaboration can be integrated for content production, digital supply chains, media streaming, broadcast playout, analytics, and archive.

Home directories: Businesses wanting to take advantage of the scalability and cost benefits of the cloud are extending access to home directories for many of their users. Cloud file storage systems adhere to common file-level protocols and standard permissions models. Therefore, customers can lift and shift applications that need this capability to the cloud.

instance stores (block level storage) - stores data in blocks so that if information is changed only the block containing the information is changed this is good for databases, enterprse software and file systems and his how data is written to hard drive. the problem with this storage is that ec2 instances can start on any host within a region and therefore if you write data to a certain host theres no guarantee that ec2 instance will go back to the same host or server which is why if you clsoe the instance the data is deleted from that host. this storage is then good for temporary data that can be erased without issue. dont write important data to storage on ec2 instances. good for frequently updated data like app or system files. Block storage is also good for :

Transactional workloads: Organizations that process time-sensitive and mission-critical transactions store such workloads into a low-latency, high-capacity, and fault-tolerant database. Block storage allows developers to set up a robust, scalable, and highly efficient transactional database. Because each block is a self-contained unit, the database performs optimally, even when the stored data grows.

Containers: Developers use block storage to store containerized applications on the cloud. Containers are software packages that contain the application and its resource files for deployment in any computing environment. Like containers, block storage is equally flexible, scalable, and efficient. With block storage, developers can migrate the containers seamlessly between servers, locations, and operating environments.

Virtual machines: Block storage supports popular virtual machine (VM) hypervisors. Users can install the operating system, file system, and other computing resources on a block storage volume. They do so by formatting the block storage volume and turning it into a VM file system. So they can readily increase or decrease the virtual drive size and transfer the virtualized storage from one host to another.

object storage - by contrast when a file in object storage is modified the entire object is rewritten instead of just a number of blocks like with block level storage. Object storage often follows a write once read many model like with amazon s3 and bucket storage. see amazon s3 for more info

Data archiving: Cloud object storage is excellent for long-term data retention. You can cost-effectively archive large amounts of rich media content and retain mandated regulatory data for extended periods of time. You can also use cloud object storage to replace on-premises tape and disk archive infrastructure. This storage solution provides enhanced data durability, immediate retrieval times, better security and compliance, and greater data accessibility. 

Backup and recovery: You can configure object storage systems to replicate content so that if a physical device fails, duplicate object storage devices become available. This ensures that your systems and applications continue to run without interruption. You can also replicate data across multiple data centers and geographical regions.

Rich media: With object storage, you can accelerate applications and reduce the cost of storing rich media files such as videos, digital images, and music. By using storage classes and replication features, you can create cost-effective, globally replicated architecture to deliver media to distributed users. 

Ec2 instance storage - if you delete the the ec2 instance then the ec2  storage for that instance is also deleted which is why this called ephemeral storage, the life cycle of the data is tied to the lifecycle of the instance. ideal for hadoop clusters. his is preconfigured storage that exists on the same physical server that hosts the EC2 instance and cannot be detached from Amazon EC2. You can think of it as a built-in drive for your EC2 instance.

amazon elastic block store -  ebs volumes only work through ec2 instances, id your instance is stopped or terminated or there is an accident or crash, your data is saved in a snapshot. ebs allows you to take snapshots or backups of your data and is used for data that is good for storage in blocks like video files thats need to be edited from the cloud so that the entire video file doesnt need to be uploaded like an s3 storage class everytime a write is done. solid state by default with hdd options, sizes up to 16TiB. you need ot be in the same availability zone or datacenter to attach ec2 instances.if your storage is filled thats all you get within your plan. stores data in a single availability zone or data centre. you can have the same EBS volume attached to multiple instances, this is called EBS multi attach. here are the best use cases for ebs:

Operating systems: Boot and root volumes can be used to store an operating system. The root device for an instance launched from an Amazon Machine Image (AMI) is typically an EBS volume. These are commonly referred to as EBS-backed AMIs.

Databases: As a storage layer for databases running on Amazon EC2 that will scale with your performance needs and provide consistent and low-latency performance.

Enterprise applications: Amazon EBS provides high availability and high durability block storage to run business-critical applications.

Big data analytics engingines: Amazon EBS offers data persistence, dynamic performance adjustments, and the ability to detach and reattach volumes, so you can resize clusters for big data analytics.

ebs snapshot - is an incremental backup which means the first backup copies all data on disc while other increments only backup new data which is different to a full backup. stored in multiple availability zones.

amazon s3 - or amazon simple storage service is a data store for an unlimited amount of data,is an object level storage, stores objects in buckets, buckets havea flat structure so there is no heirarchy or subfolders in a bucket however by using prefixes and delimeters eg. bucket1/cats/folder1 in an object key name, the Amazon S3 console and the AWS SDKs are able to infer hierarchy and introduce the concept of folders.  a file is stored as an object max 5 tb in size, is a standalone storage not tied to compute. objects can be versioned like in github and you can give permissions to each object. you can create multiple buckets. web enabled, regionally distributed, big cost saving compared to ebs, serverless, write once read many. see s3 storage pricing here: https://aws.amazon.com/s3/pricing/.

Amazon s3 use cases:

Backup and storage - Amazon S3 is a natural place to back up files because it is highly redundant. As mentioned in the last lesson, AWS stores your EBS snapshots in Amazon S3 to take advantage of its high availability.

Media hosting - Because you can store unlimited objects, and each individual object can be up to 5 TB, Amazon S3 is an ideal location to host video, photo, and music uploads.

Software delivery - You can use Amazon S3 to host your software applications that customers can download

Data lakes - Amazon S3 is an optimal foundation for a data lake because of its virtually unlimited scalability. You can increase storage from gigabytes to petabytes of content, paying only for what you use.

Static websites - You can configure your S3 bucket to host a static website of HTML, CSS, and client-side scripts.

Static content - Because of the limitless scaling, the support for large files, and the fact that you can access any object over the web at any time, Amazon S3 is the perfect place to store static content.


Making an S3 bucket :
---------------------------------------------------------------------------------------------------------------------------

When making an s3 bucket name it has to be reachable over http and https so there can be any special characters or spaces etc. the bucket will be accessible with a url. Each bucket name must be completely unique to any other across all aws accounts in all aws regions within a partition, a partition is a grouping of regions, there are 3 partitions in aws including standard regions, china regions and aws gov cloud US. here are some rules for naming buckets in case you want to automate the process:

    Bucket names must be between 3 (min) and 63 (max) characters long.
    Bucket names can consist only of lowercase letters, numbers, dots (.), and hyphens (-).
    Bucket names must begin and end with a letter or number.
    Buckets must not be formatted as an IP address.
    A bucket name cannot be used by another AWS account in the same partition until the bucket is delete

buckets use a key signed by the key management service which costs money, dont make a bucket unless youre going to use it because just having a key for the bucket incurs charges. the s3 bucket by default can only be accessed by the creator of the bucket it is a private resource. 

To make a private bucket publically accessible by everyone on the internet:  you need to edit the "block public access" option in permissions, then you nneed to edit object ownership and click "ACLs enabled" which turns off the bucket access policy and enables ACLs anf their associated permissions to take their place. next in the object tab you need to select "make public using ACL" in the actions drop down, can only be done if you have objects in your bucket. to make access to your bucket more granular and less absolute you can use IAM policies which are useful for when you have alot of buckets with differrent permission requirements or you want all policies to be managed ina central location or you can use Bucket policies which are individual policies for single buckets, good for individual small scale use, both are defined in JSON format

having a single bucket for multiple ec2 instances: after a bucket has been made go to instances in ec2 console, check the instance you want to use then select actions, image and templates, launch more like this, make sure auto assign public IP is enabled in network settings and then finally go to advanced menu and in user data box add the bucket name, then launch an instance the same as the checked one. 

s3 storage classes:
----------------------------------------------------------------------------------------------------------------------------

s3 standard - buckets stored on this storage class have a 99.99999999999% chance of durability for 1 year. 11 nines durability. stored in at least 3 facilities all amazon s3 services use object type storage

static website hosting - putting all static website assets into a bucket and connecting them to a url which can be publically accesssed.

s3 standard infrequent access - standard infrequent access - used for data that accessed less freqeuntly but requires rapid access when needed, good for storing back up files or long term storage files.

s3 glacier flexible retrieval - retain data for several years for auditing processes this data does not need to be retrieved rapidly, data is stored in vaults and populated with archives. vaults can be given a lock policy not allowing it to be opened for several years. has write once read many (WORM) capability, lock policy can not be changed. for data accessed 1 or 2 times a year

s3 lifecycle policies - allows you to move data automatically between the above tiers for periods of days eg from glacier to standard form 90 days etc.use cases for lifecycle include: Periodic logs: If you upload periodic logs to a bucket, your application might need them for a week or a month. After that, you might want to delete them. Data that changes in access frequency: Some documents are frequently accessed for a limited period of time. After that, they are infrequently accessed. At some point, you might not need real-time access to them. But your organization or regulations might require you to archive them for a specific period. After that, you can delete them.

s3 glacier instant retrieval - for archived data that needs immediate access, but is rarely accessed. 

s3 one zone infrequent access - stores data on a single availability zone, lower storage price than s3 standard ia, which uses 3 zones. is less safe and shouldnt be used for valuable un replicable data.

s3 Glacier deep archive - able to retrieve objects within 12-48 hours, lowest cost storage class ideal for archiving, designed to store data for 7 to 10 years, can only be accessed once or twice a year.  

s3 outposts - creates s3 buckets on amazon s3 outposts. delivers object storage to your on premises aws outpost, For workloads that require satisfying local data residency requirements or need to keep data close to on premises applications for performance reasons 

s3 intelligent tiering - for data with changing access patterns, requires a small monthly monitoring and automation fee per object. monitors your data usage and places it into an s3 tier for you and will change it based on usage. Moves objects between a frequent access tier (s3 standard) and an infrequent access tier (s3 standard IA), can also actually monitor object access patterns

elastic file system EFS - is a linux file system, can be accessed from anywhere in the region, data storage automatically is scaled up in size when used without disrupting applications. is a form of file storage compared to block storage. Ideal for cases where a large number of services and resources need access to the same data at the same time, you can have efs configured for one zone or multiple zones, is preconfigured storage that exists on the same physical server that hosts the EC2 instance and cannot be detached from Amazon EC2. You can think of it as a built-in drive for your EC2 instance, is the only cloud-native shared file system with fully automatic lifecycle management. Tens, hundreds, or even thousands of compute instances can access an Amazon EFS file system at the same time
  
Amazon fsx - fully managed service, choose between 4 widely used file systems: NetApp ONTAP, OpenZFS, Window File Server and Lustre

Amazon s3 versioning - Amazon S3 identifies objects in part by using the object name. For example, when you upload an employee photo to Amazon S3, you might name the object employee.jpg and store it in a bucket called employees. Without Amazon S3 versioning, every time you upload an object called employee.jpg to the employees bucket, it will overwrite the original object. To counteract these issues, you can use Amazon S3 versioning. Versioning keeps multiple versions of a single object in the same bucket. This preserves old versions of an object without using different names, which helps with object recovery from accidental deletions, accidental overwrites, or application failures.Deleting an object does not remove the object permanently. Instead, Amazon S3 puts a marker on the object that shows that you tried to delete it. If you want to restore the object, you can remove the marker and the object is reinstated. If you overwrite an object, it results in a new object version in the bucket. You still have access to previous versions of the object.

Versioning states - Buckets can be in one of three states. The versioning state applies to all objects in the bucket. Storage costs are incurred for all objects in your bucket, including all versions. To reduce your Amazon S3 bill, you might want to delete previous versions of your objects when they are no longer needed. here are the version categories:

Unversioned: No new and existing objects in n the bucket have a version.

Verioning enabled:  Versioning is enabled for all objects in the bucket. After you version-enable a bucket, it can never return to an unversioned state. However, you can suspend versioning on that bucket.

Versioning suspended: Versioning is suspended for new objects. All new objects in the bucket will not have a version. However, all existing objects keep their object versions.

databases
=============================================================================================================================

Database types - 

Ledger database: for systems of record, supply chain , registration, banking transactions use amazon ledger database service QLDB

Time series database:

used for internet of things applications, devops, industrial telemetry use amazon timestream

Graph database: 

used for fraud detection, social networking, recommendation engines use amazon neptune

Wide column database: 

for high scale industrial apps for equipment maintenance, fleet management, and route optimization use amazon keyspaces, good for apache cassandra workloads.

Document database:

for content management, catalogs and user profiles use Amazon document DB wit mongoDB compatibility, for JSON workloads 

In memory database:

for caching, session management, gaming leaderboards, geospatial applciations use Amazon elasticache, Amazon memory DB for Redis

Key Value database: for high traffic webb applictions, e-commerce systems, gaming applications use Amazon Dynamo DB.

relational databases: A relational database organizes data into tables. Data in one table can link to data in other tables to create relationships—hence, the relational part of the name. A table stores data in rows and columns. A row, often called a record, contains all information about a specific entry. Columns describe attributes of an entry. The tables, rows, columns, and relationships between them is called a logical schema. With relational databases, a schema is fixed. After the database is operational, it becomes difficult to change the schema. Because of this, most of the data modeling is done up front before the database is active.  

used in traditional applications, including enterprise resource planning ERP, customer relationship management CRM and e-commerce. AWS service of use in this case include amazon aurora, RDS, Redshift. Amazon rds is compatible with multiple engines, can be launched in multiple AZ configuration to be deployed for high availability HA.

use SQL to store query data, amazon rds service enables relational databases to be run in the cloud. amazon rds provisions hardware, db setup, patching and backups.
as well as customer ownership of schema and ownership of data

relational database management system or RDBMS: used to create, update, and administer a relational database. Some common examples of RDBMSs include the following:

    MySQL
    PostgresQL
    Oracle
    Microsoft SQL Server
    Amazon Aurora

You communicate with an RDBMS by using structured query language (SQL) queries such as SELECT*FROM table_name. 
This query selects all the data from a particular table. However, the power of SQL queries is in creating more complex queries that pull data from several tables to identify patterns and answers to business problems. For example, querying the sales table and the books table together to see sales in relation to an author’s books. Querying tables together to better understand their relationships is made possible by a "join". Relational databases ensure that your data has high integrity and adheres to the atomicity, consistency, isolation, and durability (ACID) principle. some common use casesfor rdbms include:

Applications that have a fied schema:  These are applications that have a fixed schema and don't change often. An example is a lift-and-shift application that lifts an app from on-premises and shifts it to the cloud, with little or no modifications.

Applications that need fixed storage:  These are applications that need persistent storage and follow the ACID principle, such as:

    Enterprise resource planning (ERP) applications
    Customer relationship management (CRM) applications
    Commerce and financial applications

managed vs unmanaged databases: where amazon takes care of  the following: Os installation, server maintenance, rack and stack, power, HVAC and net as well all costs and insurance involved with securing and maintaining physical hardware and OS. most other things including App optimization, scaling, high availability, db backups, DB software patches, db software installs and OS patches are up to you!

amazon rds: with rds you can scale components of the service meaning you can increase or decrease specific db configurations independently. Amazon RDS is a managed database service customers can use to create and manage relational databases in the cloud without the operational burden of traditional database management amazon rds supported db engines: amazon aurora, postgre sql, mysql, mariaDB, oracle, microsoft sql server. RDS uses EBS for storage and has 3 storage types: General Purpose SSD (also called gp2 and gp3), Provisioned IOPS SSD (also called io1) for high levels of IO, and Magnetic (also called standard) for backward compatibility. They differ in performance characteristics and price. charges oer hour of runtime 

With rds auto backups are turned by default and should be setup to backup during times of low activity backups are retained for up to 35 days, setting this to 0 means no backups are done and also all existing backups are deleted. manual snapshots are also an option and can be kept for longer than 35 days

rds multi az - In an Amazon RDS Multi-AZ deployment, Amazon RDS creates a redundant copy of your database in another Availability Zone. You end up with two copies of your database—a primary copy in a subnet in one Availability Zone and a standby copy in a subnet in a second Availability Zone. The primary copy of your database provides access to your data so that applications can query and display the information. The data in the primary copy is synchronously replicated to the standby copy. The standby copy is not considered an active database, and it does not get queried by applications. All done to improva availability. To help ensure that you don't lose Multi-AZ configuration, there are two ways you can create a new standby database. They are as follows:

    Demote the previous primary to standby if it's still up and running.
    Stand up a new standby DB instance.

The reason you can select multiple subnets for an Amazon RDS database is because of the Multi-AZ configuration. You will want to ensure that you have subnets in different Availability Zones for your primary and standby copies.

amazon rds security - Use IAM policies to assign permissions that determine who can manage Amazon RDS resources. For example, you can use IAM to determine who can create, describe, modify, and delete DB instances, tag resources, or modify security groups. Use security groups to control which IP addresses or Amazon EC2 instances can connect to your databases on a DB instance. When you first create a DB instance, all database access is prevented except through rules specified by an associated security group. Use Amazon RDS encryption to secure your DB instances and snapshots at rest. Use Secure Sockets Layer (SSL) or Transport Layer Security (TLS) connections with DB instances running the MySQL, MariaDB, PostgreSQL, Oracle, or SQL Server database engines. 

amazon aurora - compatible with mysql and postgre sql and is 5 times faster than standard mysql dbs. amazon aurora is an enterprise class relational database.replicates 6 copies of your data to 3 data centers and continuosly baks up to amazon s3, performs at 1/10th cost of regulat dbs. 

non relational database:

dynamo db - made up of standalone tables instead of relational tables, fully managed, data stred on ssd,  a key value database service, serverless database meaning it doesnt require any management of physical resources or instances. is redundant across multiple datacentres. is non relational meaning it doesnt use sql tables to store data. sql databases are usually quite rigid and hard to scale because of all the relations in the db but non relational dbs which have much faster query times, attributes within a dynamo db table can be removed and changed unlike sql dbs. uses key value pairs aka items and pairs which can be removed or added anytime, can scale up to 10 trillion requests per day,is great for single table databases, tight integration with infrastructure as code IAC. has become the db of choice for high scale applications and serverless applications, works well with online transaction processing OLTP

Dynamo db is made of 3 core attributes tables: Similar to other database systems, DynamoDB stores data in tables. A table is a collection of data. For example, you can have a table called Person that you can use to store personal contact information about friends, family, or anyone else of interest. You can also have a Cars table to store information about vehicles that people drive., items: Each table contains zero or more items. An item is a group of attributes that is uniquely identifiable among all the other items. In a Person table, each item represents a person. In a Cars table, each item represents one vehicle. Items in DynamoDB are similar in many ways to rows, records, or tuples in other database systems. In DynamoDB, there is no limit to the number of items you can store in a table, attributes: Each item is composed of one or more attributes. An attribute is a fundamental data element, something that does not need to be broken down any further. For example, an item in a Person table might contain attributes called PersonID, LastName, FirstName, and so on. In a Department table, an item might have attributes such as DepartmentID, Name, Manager, and so on. Attributes in DynamoDB are similar in many ways to fields or columns in other database systems. use cases for dynamo db include:

You are experiencing scalability problems with other traditional database systems.
You are actively engaged in developing an application or service.
You are working with an OLTP workload.
You care deploying a mission-critical application that must be highly available at all times without manual intervention.
You require a high level of data durability, regardless of your backup-and-restore strategy.
create media metadata stores, software devbelopment, scale gaming platforms, deliber seamless retail experiences

Dynamo db security features are much the same as every other storage offering including encryption at rest, AWS KMS, redundant storage across multiple AZ,IAM management, aws global network security procedures

data warehouses:

looks at data for historical analytics as opposed to operational analysis. historical data is data that is set in the past and wont be changed like the sale figuresfor 2nd july 2023 as opposed to "how many bags of coffee do i have left right now ?" after so many sales.

amazon redshift -  allows you run a single query across exabytes of unstrctured historical data  at a rate up to 10 times higher than traditional sql dbs, columnar storage, can run online analytical processing workload OLAP.

amazon database migration service (adms) - allows source database to remain fully operational during migration. the source and target dbs dont have to be the same type. homogenous migrations are migrations between dbs of the same type eg oracle, sql, heterogenous migrations are db migrations between dbs of different type that have diffeerent schema structures, data types, and db code. this data 1st needs to be converted with aws schema conversion tool which converts all data so that it can fit with the schema of the new db. is useful also for dev and test db migrationwhich is the copying of db data to a new db for testing purposes so that the original db can still be used for its original purpose, db consolidation where you have multiple dbs and want to consolidate them into a single db, and continuous db replication which is making multiple copies of the same db 

other databases;

Amazon document db -  has mongo db compatibility is useful for content management, catalogs, user profiles. A document database is a type of NoSQL database you can use to store and query rich documents in your application. These types of databases work well for the following use cases: content management systems, profile management, and web and mobile applications. Amazon DocumentDB has API compatibility with MongoDB. This means you can use popular open-source libraries to interact with Amazon DocumentDB, or you can migrate existing databases to Amazon DocumentDB with minimal hassle.

amazon neptune - a graph db engineered for social networking, recommendation engines and fraud detection, knowledge graphs. A graph database is a good choice for highly connected data with a rich variety of relationships.

amazon managed block chain - good for supply chain tracking, banking and financial records that require 100% immutability, crypto. blockchain is a distributed ledger system that lets multiple parties run transactions and share data without a central authority.

amazon quantum ledger db ( QLDB ) - good for the above use cases and is regarded as an immutable system of record thay can never be removed for audits

amazon keyspaces:  scalable, highly available, and managed Apache Cassandra compatible database service. Amazon Keyspaces is a good option for high-volume applications with straightforward access patterns. With Amazon Keyspaces, you can run your Cassandra workloads on AWS using the same Cassandra Query Language (CQL) code, Apache 2.0 licensed drivers, and tools that you use today.

amazon timestream: Timestream is a fast, scalable, and serverless time series database service for Internet of Things (IoT) and operational applications. It makes it easy to store and analyze trillions of events per day up to 1,000 times faster and for as little as one-tenth of the cost of relational databases. Time series data is a sequence of data points recorded over a time interval. It is used for measuring events that change over time, such as stock prices over time or temperature measurements over time.

amazon quantam ledger database qldb: With traditional databases, you can overwrite or delete data, so developers use techniques, such as audit tables and audit trails to help track data lineage. These approaches can be difficult to scale and put the burden of ensuring that all data is recorded on the application developer. Amazon QLDB is a purpose-built ledger database that provides a complete and cryptographically verifiable history of all changes made to your application data.

database accelerators:

amazon elasticache: can increase the caching of your db system to reduce read times, comes in memcache D and redis flavours. You aren’t responsible for instance failovers, backups and restores, or software upgrades.

amazon memorydb for redis: MemoryDB is a Redis-compatible, durable, in-memory database service that delivers ultra-fast performance. With MemoryDB, you can achieve microsecond read latency, single-digit millisecond write latency, high throughput, and Multi-AZ durability for modern applications, like those built with microservices architectures. You can use MemoryDB as a fully managed, primary database to build high-performance applications. You do not need to separately manage a cache, durable database, or the required underlying infrastructure.

Amazon dynamo db accelerator - caching performance options for dynamo db


Security
==================================================================================

AWS shared responsibility model - customer has responsibility inside the cloud eg. identity and access management, os network and firewall config, data encryption, and network traffic protection eg. encryption and scheduled backups also Choosing a Region for AWS resources in accordance with data sovereignty regulations. AWS has responsibility for the cloud infrastrucure security susch the hardware, storage, regions, edge locations, the virtualization layer and so on. aws can provide domcumentation for security compliance with a variety of computer security standards and regulations. your operations team is 100% reponsible for patching vulnerabilities, amazon can notifiy you of important patches but will not be the ones who configure your system.

types of MFA multi factor authentication - 

Virtual MFA: A software app that runs on a phone or other device that provides a one-time passcode. These applications can run on unsecured mobile devices, and because of that, they might not provide the same level of security as hardware or FIDO security keys. supported devices include: Twilio Authy Authenticator, Duo Mobile, LastPass Authenticator, Microsoft Authenticator, Google Authenticator, Symantec VIP

Hardware TOTP token: A hardware device, generally a key fob or display card device, that generates a one-time, six-digit numeric code based on the time-based one-time password (TOTP) algorithm. suppported devices include: Key fob, display card

FIDO security keys: FIDO-certified hardware security keys are provided by third-party providers such as Yubico. You can plug your FIDO security key into a USB port on your computer and enable it using the instructions that follow. supported devices are any FIDO certified products

user permissions and access - aws account owner is also the root user who has supreme access of everything. A user should be created immediately to perform most tasks. Only access root user for a limited number of tasks like changing root user email and changing aws support plan. for example a bad actor can steal your root user permissions and transform your account into a crypto miner. Multi factor auth should be enabled to mitigate this possibility. 

aws id and access management - where you create IAM users and give them permissions. all actions are denied by default and need to be explicitly given access to each user, called the principle of least privilege. IAM policy is described in a JSONfile. IAM groups allow to distribute the same priviliges to a large number of people and each group can have different permissions. roles give a different set of permissions to a user based on the tasks that are performed on that day or the job role they have, used for temporary tasks, sheds previous permissions for the role permissions, is good for employees who rotate through different tasks each day, role priviliges usuallu expire within a time period of 15 minutes to 36 hours. IAM also handles multi factor authentication. Every user should be given an individual account and accounts should not be shared this increases security types of MFA multi factor authentication. IDP or identity providers is a way to manage employee identity information on a large scale eg. 300 + employees. IDP services in AWS include IAM identity center or 3rd party identity providers which stops you from creating separate IAM users for each user in AWS instead you can asswign an IAM role to provide permission to identities that are federated from your idp.

IAM role can be used to authenticate users in the corporate network without signing in

IAM policy examples

Most policies are stored in AWS as JSON documents with several policy elements. The following example provides admin access through an IAM identity-based policy.

{
"Version": "2012-10-17",
"Statement": [{
"Effect": "Allow",
"Action": "*",
"Resource": "*"
}]
}

This policy has four major JSON elements: Version, Effect, Action, and Resource. The Version element defines the version of the policy language. It specifies the language syntax rules that are needed by AWS to process a policy. To use all the available policy features, include "Version": "2012-10-17" before the "Statement" element in your policies. The Effect element specifies whether the policy will allow or deny access. In this policy, the Effect is "Allow", which means you’re providing access to a particular resource. The Action element describes the type of action that should be allowed or denied. In the example policy, the action is "*". This is called a wildcard, and it is used to symbolize every action inside your AWS account. The Resource element specifies the object or objects that the policy statement covers. In the policy example, the resource is the wildcard "*". This represents all resources inside your AWS console.
Putting this information together, you have a policy that allows you to perform all actions on all resources in your AWS account. This is what we refer to as an administrator policy.

The next example shows a more granular IAM policy.

{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Sid": "DenyS3AccessOutsideMyBoundary",
      "Effect": "Deny",
      "Action": [
        "s3:*"
      ],
      "Resource": "*",
      "Condition": {
        "StringNotEquals": {
          "aws:ResourceAccount": [
            "222222222222"
          ]
        }
      }
    }
  ]
}

Federated: a process that allows for the transfer of identity and authentication information across a set of networked systems.   

Virtual MFA: A software app that runs on a phone or other device that provides a one-time passcode. These applications can run on unsecured mobile devices, and because of that, they might not provide the same level of security as hardware or FIDO security keys. supported devices include: Twilio Authy Authenticator, Duo Mobile, LastPass Authenticator, Microsoft Authenticator, Google Authenticator, Symantec VIP

Hardware TOTP token: A hardware device, generally a key fob or display card device, that generates a one-time, six-digit numeric code based on the time-based one-time password (TOTP) algorithm. suppported devices include: Key fob, display card

FIDO security keys: FIDO-certified hardware security keys are provided by third-party providers such as Yubico. You can plug your FIDO security key into a USB port on your computer and enable it using the instructions that follow. supported devices are any FIDO certified producttypes of MFA multi factor authentication - 

Virtual MFA: A software app that runs on a phone or other device that provides a one-time passcode. These applications can run on unsecured mobile devices, and because of that, they might not provide the same level of security as hardware or FIDO security keys. supported devices include: Twilio Authy Authenticator, Duo Mobile, LastPass Authenticator, Microsoft Authenticator, Google Authenticator, Symantec VIP

Hardware TOTP token: A hardware device, generally a key fob or display card device, that generates a one-time, six-digit numeric code based on the time-based one-time password (TOTP) algorithm. suppported devices include: Key fob, display card

FIDO security keys: FIDO-certified hardware security keys are provided by third-party providers such as Yubico. You can plug your FIDO security key into a USB port on your computer and enable it using the instructions that follow. supported devices are any FIDO certified producttypes of MFA multi factor authentication - 

Virtual MFA: A software app that runs on a phone or other device that provides a one-time passcode. These applications can run on unsecured mobile devices, and because of that, they might not provide the same level of security as hardware or FIDO security keys. supported devices include: Twilio Authy Authenticator, Duo Mobile, LastPass Authenticator, Microsoft Authenticator, Google Authenticator, Symantec VIP

Hardware TOTP token: A hardware device, generally a key fob or display card device, that generates a one-time, six-digit numeric code based on the time-based one-time password (TOTP) algorithm. suppported devices include: Key fob, display card

FIDO security keys: FIDO-certified hardware security keys are provided by third-party providers such as Yubico. You can plug your FIDO security key into a USB port on your computer and enable it using the instructions that follow. supported devices are any FIDO certified products
  
aws organizations - a central location to manage multiple aws accounts, manage billing of all accounts, where you groups accounts into organizational units, where you give accounts access to aws services and api actions. service control policies or scp's specifiy maximum permissions for member accounts in the organization, this service sits above IAM. aws organizations is the root of all your aws accounts.


compliance 
---------------------------------------------------

if you deal with consumer data in the EU you will need to be in compliance with the general data protection regulation GDPR, or if you run health care apps in the USA you will need to be in compliance with the Health insurance portability and accountability act HIPAA. Check AWS compliance programs online, they show that aws infrastructure already meets compliance its up to the user to reach the rest of those requirements on their end.

AWS Artifact - gain access to compliance reports done by 3rd parties, artifact agreements is where you would sign compliance agreements online, Artifact reports is where you would check if your app is in compliance
AWS compliance centre - where you find compliance information

different kinds of online attack and solutions
---------------------------------------------------

DDOS - distributed denial of service. in a DDOS attack the bad actor tries to overwhelm the capacity of your application and deny service to your customer, it does this by distributing this task to multiple bot computers that are all used together to overwhelm your application. Examples of DDOS attacks:

UDP flood - A low level network attack. a bad actor for example could leverage the weather service bot computers to flood your app infrastructure with weather data by giving the weather service your ip address as the reutn address for its request. 

A solution to this is security groups, which monitor incoming traffic protocols where if its weather report data it will be denied access, this works on the network infrastructure level which leverages the entire aws local network capacity.

HTTP level attacks - are where seemingly normal users are making normal requests of your network, however they are actually bot computers and there are so many of them that they stop real users from using your app

slowloris attack - attacker pretends to have a terribly slow connection and very slowly sends a request packet through terrible bandwidth to  your system. this halts your system and stops it from serving other users. this attack only requires a few slowloris attackers to work

A solution to the slowloris attack would be the Elastic load balancer which is a middle man to the front end of the system this stops the attack from reaching the front end, it is scalable and runs at the region level so it can withstand a large bruteforce attack

AWS shield with AWS web application firewall WAF - WAF  monitors network requests for web applications, filters incoming web traffic for the signatures of bad actors using a web access control list WACL, has machine learning capabibilities and recognizes new threats as they evolve. AWS WAF also utilizes other AWS tools like elastic load balancer, route 53 and AWS WAF and cloud front to achieve this, also comes with detailed diagnostic tools and the ability to write your own custom rules to mitigate attacks. AWS shield protects aplications against ddos. 

AWS key management service - encryption - securing a message or data in a way that only authorized parties can access it through cryptographic keys. there are 2 types of encryption: encryption at rest which is encryption for stored data like in dynamo DB, keys used to encrypt your data are generated and managed in AWS KMS, this is where you can also disable keys and manage acccounts that have access to keys. the 2nd type is encryption in transit, which utilizes secure socket layer ssl, and service certificates to validate and authorize clients.

Amazon inspector - runs automated security assessments against your security infrastructure to improve security and compliance of your aws deployed apps, it will provide a list of security findings and how to fix them.

amazon guard duty - provides intelligent threat detection for aws infrastructure and resources. analyzes metadata generated from your account and network activity found in AWS cloud trail , amazon vpc flow logs and DNS logs. can be added to your ec2 instance. wont effect performance


Monitoring
================================================================================== 

Metrics - Each individual data point that a resource creates is a metric. Think of a metric as a variable to monitor and the data points as representing the values of that variable over time. Every metric data point must be associated with a timestamp. 

Metrics in amazon s3 include: 

Size of objects stored in a bucket
Number of objects stored in a bucket
Number of HTTP request made to a bucket

Amazon relational database metrics include:

    Database connections
    CPU utilization of an instance
    Disk space consumption

Amazon ec2 metrics include:

    CPU utilization
    Network utilization
    Disk performance
    Status checks

request latency
error response rate 

If you want to gain more granular visibility, you can use high-resolution custom metrics, which make it possible for you to collect custom metrics down to a 1-second resolution. This means you can send 1 data point per second per custom metric.

Some examples of custom metrics include the following:

Webpage load times
Request error rates
Number of processes or threads on your instance
Amount of work performed by your application

benefits of monitoring include responding proactively to problems before they become endemic, waiting for end users to let you knwo when your application is experiencing an outage is a bad practice. Imrpoving the reliability of your resources, monitoring can illuminate bottlenecks and inefficient architectures. recognize security threats by establishing a baseline of normal network atctivity so that when there are spikes in activity or unusual IP addresses accessing resources you can jump on them. you cna make data driven decisions by collecting app level data viewing the number of users who use new features of the app with this kowledge you will know how to improve the feature. monitoring can also help you make more cost effective solutions by re utilizing underused resources.  

Amazon cloudwatch - enables you to monitor and manage various metrics and and configure alarms based on data from those metrics, cloudwatch can also automatically create graphs to show how these metrics have performed over time. Cloudwatch alarms when set can also automatically perform actions if the value has gone above or below a predefined threshold. for example you can configure your ec2 instances to shutdown if the cpu hasnt been utilized for a certain amount of time, thus saving you money, alarms can also send notifications by sms or email or whatever else.the cloudwatch dashboard is customizable and able to show you all kinds of metrics over time. monitors your resource utilization and performance.

cloudwatch costs money ! - Many AWS services automatically send metrics to CloudWatch for free at a rate of 1 data point per metric per 5-minute interval. This is called basic monitoring, and it gives you visibility into your systems without any extra cost. For many applications, basic monitoring is adequate. For applications running on EC2 instances, you can get more granularity by posting metrics every minute instead of every 5-minutes using a feature like detailed monitoring. Detailed monitoring incurs a fee. For more information about pricing, see "Amazon CloudWatch Pricing" in the Resources section at the end of this lesson.

AWS services that send data to CloudWatch attach dimensions to each metric. A dimension is a name and value pair that is part of the metric’s identity. You can use dimensions to filter the results that CloudWatch returns. For example, many Amazon EC2 metrics publish InstanceId as a dimension name and the actual instance ID as the value for that dimension.
Screenshot depicting the metrics and dimensions used to filter the results that CloudWatch returns. By default, many AWS services provide metrics at no charge for resources such as EC2 instances, Amazon Elastic Block Store (Amazon EBS) volumes, and Amazon RDS database (DB) instances. For a charge, you can activate features such as detailed monitoring or publishing your own application metrics on resources such as your EC2 instances. 

cloudwatch dashboard - CloudWatch aggregates statistics according to the period of time that you specify when creating your graph or requesting your metrics. You can also choose whether your metric widgets display live data. Live data is data published within the last minute that has not been fully aggregated. You are not bound to using CloudWatch exclusively for all your visualization needs. You can use external or custom tools to ingest and analyze CloudWatch metrics using the GetMetricData API.

cloudwatch logs - With CloudWatch Logs, you can query and filter your log data. For example, suppose you’re looking into an application logic error for your application. You know that when this error occurs, it will log the stack trace. Because you know it logs the error, you query your logs in CloudWatch Logs to find the stack trace. You also set up metric filters on logs, which turn log data into numerical CloudWatch metrics that you can graph and use on your dashboards.
Some services, like Lambda, are set up to send log data to CloudWatch Logs with minimal effort. With Lambda, all you need to do is give the Lambda function the correct IAM permissions to post logs to CloudWatch Logs. Other services require more configuration. For example, to send your application logs from an EC2 instance into CloudWatch Logs, you need to install and configure the CloudWatch Logs agent on the EC2 instance. With the CloudWatch Logs agent, EC2 instances can automatically send log data to CloudWatch Logs. cloudwatch groups are groups of streams, cloudwatch streams are streams of events, cloudwatch events are records of activity recorded by the application or resource being monitored. It has a timestamp and an event message.

cloudwatch alarms - setup to notify you when your infrastructure is having capacity, performance or availability issues. an alarm can be triggered when it transitions from one fo the following states to another aftr an alarm is triggered it can initiate an action eg notifiy through SNS. Alarm states are "OK" where operations are within alarm threshold, "alarm" where operations are outside of defined threshold and "insufficient_data" where the alarm has only just started.

Amazon cloudtrail - records API calls for your account, recorded information includes api caller identity, time of call, source ip address of caller. this is a log that records the who, how, when, where of an api caller, everything except the why?. recall that api calls are used to do everything in aws including provisioning, managing and configuring your aws resources. cloud trail allows you to review a complete history of this activity, updated within 15 minutes of api call being made.cloud trail insights is an optional feature that can automatically detect unusual api activities in your account, for example insights might detect that a higher number of ec2 instances have been launched than what is normal and it will alert you

AWS trusted advisor - inspects your aws environment and provides real time recommendations according to aws best practices across 5 categories: cost, optimization , performance: eg. provides recommendations for how to take advantage of provisioned throughput, security, fault tolerance and service limits. It will do this by issuing green, orange and red checks across these 5 categories, green means no problem detected, orange means that some investigations are recommended and a red circle means actions need to be performed immediately to illeviate poor performance, functionality or efficiency in any of the categories, can for example review security of your amazon s3 buckets by checking for open access permissions

trusted advisor can help save costs by identifying RDS DB instances, underutilized EBS volumes, unassociated elastic IP addresses and excessivr timeouts in Lambda functions. it can help performance by analyzing EBS throughput and latency, EC2 compute usage and cloudfront configurations. it can improve security by identifying RDS securoty group access risk, exposed access keys, and unneccessary s3 bucket permissions. it can improve fault tolerance by examining auto scaling ec2 groups, delted healt checks on route 53, disabled availability zones and disabled rds backups. it will also notify when you have reached 80% of your service quota 


Amazon costs
=================================================================================

EC2 instance savings plans - reduces compute costs by committing to a consistent hourly spend for a 1-year or 3-year term. This results in savings of up to 72% over On-Demand Instance costs. Any EC2 usage up to the commitment is charged at the discounted Savings Plan rate (for example, $10 an hour). Any EC2 usage beyond the commitment is charged at regular On-Demand Instance rates.

AWS free tier - enables you to begin certain services without incurring costs for a specified period, three types of free tier offers are available including always free, 12 months free and trials, 12 months free is only available to new amazon account holders.

AWS pricing - there are 3 categories, pay for what you use where you pay only for the resources that you use without requiring long term contracts and licensing. pay less when you reserve which allows you to save up to 72 % on costs compared to on demand by reserving resources for a contracted period of 1 or 3 years and volume based discounts which based on the volume of usage will incur higher and higher discounts for example with s3 storage which decreases in cost when you reach a higher tier of usage.

on demand instances - charges per hour/second, short term, unpredictable workloads, no long term comitments or upfront payments, billing begins whenever instance is running, choice ot increase or decrease compute capacity

reserved instances/savings plans - discount for 1 to 3 year commitments. standard reserved instance allows modification of availability zone, scope network type and instance size with the same instance type. Convertible reserved instance allows exchange of one or more convertible reserved instances with a different config including instance family, OS and tenency. there are no limits to how often an exchange can be performed as long as the target convertible reserved instance is of equal or greater value than the previous. Applies to EC2, Lambda and fargate. for workloads that have consistent and steady usage. instances need to be reserved or scheduled

spot instances - for applications with flexible start and end times, urgent computing needs for large capacity, up to 90% discount. Utilizes spare compute capacity. recommended for users with fault tolerant or stateless workloads. With Spot Instances, you set a limit on how much you want to pay for the instance hour. This is compared against the current Spot price that AWS determines. Spot Instance prices adjust gradually based on long-term trends in supply and demand for Spot Instance capacity. If the amount that you pay is more than the current Spot price and there is capacity, you will receive an instance.         

dedicated hosts - A Dedicated Host is a physical Amazon EC2 server that is dedicated for your use. Dedicated Hosts can help you reduce costs because you can use your existing server-bound software licenses, such as Windows Server, SQL Server, and Oracle licenses. And they can also help you meet compliance requirements. Amazon EC2 Dedicated Host is also integrated with AWS License Manager, a service that helps you manage your software licenses, including Microsoft Windows Server and Microsoft SQL Server licenses. Dedicated Hosts can be purchased on demand (hourly).
Dedicated Hosts can be purchased as a Reservation for up to 70 percent off the On-Demand price.

AWS pricing calculator - explore aws services and create estimates for the cost of your use cases and cost centres. plan your system and resources then calculate how much this systen will cost 

aws billing dashboard - used to pay your aws bill, monitor usage and analyze and control costs, compare previous months costs to this months on going costs and forecast what your costs will be for the upcoming bill by each service your are using, purchase and manage savings plans, publish usage reports, access cost explorer and create budgets

consolidated billing - through aws organizations, a service that enables you to manage multiple aws accounts from a central location, you can access consolidated billing which allows you to recive a single bill for all your aws accounts, default maxmum accounts for your organization is 4 but can be increased. the greatest benefit of consolidation is shared bulk discount pricing across all accounts, for example one account might not reach the next tier to get a discount on s3  but 3 accounts with 4tb each of transfers can reach the 10 tb tier and recieve a discount 

aws budgets -  where you can create budgets to plan your service usage, costs and instance reservations. updates 3 times a day. here you can also set custom alerts when your usage exceeds or is forecasted to excedd the budget amount for example you can set a budget to alert you when you have eaten through $100 of your $200 ec2 budget allowing you to forecast your upcomig costs and adjust your resource usage accordingly to fall within free usage limits.

aws cost explorer - lets you visualize, and manage your aws costs and usage over time, includes a default report of the costs and usage of your top 5 cost accruing aws services, where you can apply custom filters and groups to analyze your data at an houlry level for example

aws support plans - offers 4 different support plans to help you troubleshoot issues, lower costs, improve efficiency in all of your aws services. you cna choose fropm the following support plans: basic, developer, business, enterprise on-ramp and enterprise. Basic support is free for all customers it includes access to whitepapers, documentation, support communities, you can also contact aws from here for billing questions and service limit increases. you have access to a limited selection of trusted advisor checks, you can also use the aws personal health dashboard  which provides alerts to and guidance when aws is experiencng events that may effect you. 
With developer, business, enterprise on ramp and enterprise support you get all the benefits of basic support in addition to opening an unrestricted number of technical support cases. these support plans are pay by month and require no long term contracts and each represents an increased level of support and cost from developer to enterprise. the features are as follows: 

developer support : Best practice guidance,  Client-side diagnostic tools, Building-block architecture support, which consists of guidance for how to use AWS offerings, features, and services together  

business support:     Use-case guidance to identify AWS offerings, features, and services that can best support your specific needs, All AWS Trusted Advisor checks, Limited support for third-party software, such as common operating systems and application stack components

enterprise on-ramp support: A pool of Technical Account Managers to provide proactive guidance and coordinate access to programs and AWS experts, A Cost Optimization workshop (one per year), A Concierge support team for billing and account assistance, Tools to monitor costs and performance through Trusted Advisor and Health API/Dashboard, Consultative review and architecture guidance (one per year), Infrastructure Event Management support (one per year), Support automation workflows, 30 minutes or less response time for business-critical issues

enterprise support: A designated Technical Account Manager to provide proactive guidance and coordinate access to programs and AWS experts, A Concierge support team for billing and account assistance, Operations Reviews and tools to monitor health, Training and Game Days to drive innovation, Tools to monitor costs and performance through Trusted Advisor and Health API/Dashboard, Consultative review and architecture guidance, Infrastructure Event Management support, Cost Optimization Workshop and tools, Support automation workflows, 15 minutes or less response time for business-critical issues

technical account manager TAM: enterprise on ramp and and entprise support plans access to a TAM who is your primary point of contact at AWS. theu provide support, guidance, education on aws services, engineering guidance to help you design your soltions 

aws marketplace: a digital catalog contaning thousands of software listings from independent vendors where you can find, test and buy software that runs on AWS, you can explre software solutions by industry and use case like the healthcare industry where you can buy software to help with protecting patient records or a machine learrning software to analyze patients medicval history and predict possible risks. marketplace has several categories including infrastructure software, devops , data products, professional services, business applications, machine learning , industries, internet of things, and within each are sub categories that further drill down to exactly what you would need in each field.

amazon free tier: visit https://aws.amazon.com/free/?all-free-tier.sort-by=item.additionalFields.SortRank&all-free-tier.sort-order=asc&awsf.Free%20Tier%20Types=tier%23always-free&awsf.Free%20Tier%20Categories=*all, free options include always free, free trials and 1 year free.

Migration and innovation in AWS cloud
================================================================================= 

aws cloud adoption framework - organizes guidance into 6 focus areas called perspectives, each of which addresses distinct responsibilities to help carefully prepare for the task of adopting the aws cloud. Generally the business, people and governance perspectives focus on business capabilities whereas platform, security and operation perspective focus on technical capabilities, here more info on the different perspectives:

business perspective: ensures that IT aligns with business needs and that IT investments link to key business results,The Business Perspective helps you to move from a model that separates business and IT strategies into a business model that integrates IT strategy, common roles in business perspective include:Business managers, Finance managers, Budget owners, Strategy stakeholders 

People perspective:The People Perspective helps Human Resources (HR) employees prepare their teams for cloud adoption by updating organizational processes and staff skills to include cloud-based competencies. Use the People Perspective to evaluate organizational structures and roles, new skill and process requirements, and identify gaps. This helps prioritize training, staffing, and organizational changes. Common roles in the People Perspective include: Human resources, Staffing, People managers

governance perspective: Use the Governance Perspective to understand how to update the staff skills and processes necessary to ensure business governance in the cloud. Manage and measure cloud investments to evaluate business outcomes.Common roles in the Governance Perspective include: Chief Information Officer (CIO), Program managers, Enterprise architects, Business analysts, Portfolio managers

platform perspective: Helps you design, implement, and optimize your AWS infrastructure based on your business goals and perspectives. Use a variety of architectural models to understand and communicate the structure of IT systems and their relationships. Describe the architecture of the target state environment in detail.Common roles in the Platform Perspective include: Chief Technology Officer (CTO), IT managers, Solutions architects

security perspective: helps to structure and implement permissions, identify areas of non compliance. The Security Perspective ensures that the organization meets security objectives for visibility, auditability, control, and agility. Common roles in the Security Perspective include: Chief Information Security Officer (CISO), IT security managers, IT security analysts

operations perspective: focus on recovering IT workloads to meet requirements of business stakeholders,  Define how day-to-day, quarter-to-quarter, and year-to-year business is conducted. Align with and support the operations of the business. The AWS CAF helps these stakeholders define current operating procedures and identify the process changes and training needed to implement successful cloud adoption. Common roles in the Operations Perspective include: IT operations managers, IT support managers,The Operations Perspective focuses on operating and recovering IT workloads to meet the requirements of your business stakeholders.
--------------------------------------------------------------------------------
Migration strategies - there are 6 migration strategies, the 6 R's:
 
Rehosting:also known as “lift-and-shift” involves moving applications without changes. In the scenario of a large legacy migration, in which the company is looking to implementits migration and scale quickly to meet a business case, the majority of applications are rehosted.   

replatforming: also known as “lift, tinker, and shift,” involves making a few cloud optimizations to realize a tangible benefit. Optimization is achieved without changing the core architecture of the application.
 
refactoring/rearchitecting: involves reimagining how an application is architected and developed by using cloud-native features. Refactoring is driven by a strong business need to add features, scale, or performance that would otherwise be difficult to achieve in the application’s existing environment.

repurchasing: involves moving to a different product eg. moving from a traditional license to a software-as-a-service model. For example, a business might choose to implement the repurchasing strategy by migrating from a customer relationship management (CRM) system to Salesforce.com. 

retaining: consists of keeping applications that are critical for the business in the source environment. This might include applications that require major refactoring before they can be migrated, or, work that can be postponed until a later time.

retiring: is the process of removing applications that are no longer needed.

--------------------------------------------------------------------------------
AWS snow family - The AWS Snow Family is a collection of physical devices that help to physically transport up to exabytes of data into and out of AWS. All of these options have anti tamper technology, and security features, and even go as far as to have security teams and surveillance with snow mobile for example. AWS Snow Family is composed of AWS Snowcone, AWS Snowball, and AWS Snowmobile. 

Aws Snow Cone: is a small, rugged, and secure edge computing and data transfer device. 
It features 2 CPUs, 4 GB of memory, and up to 14 TB of usable storage.

AWS snow ball: offers two types of devices:
Snowball Edge Storage Optimized devices are well suited for large-scale data migrations and recurring transfer workflows, in addition to local computing with higher capacity needs. Storage: 80 TB of hard disk drive (HDD) capacity for block volumes and Amazon S3 compatible object storage, and 1 TB of SATA solid state drive (SSD) for block volumes. Compute: 40 vCPUs, and 80 GiB of memory to support Amazon EC2 sbe1 instances (equivalent to C5).

Snowball Edge Compute Optimized provides powerful computing resources for use cases such as machine learning, full motion video analysis, analytics, and local computing stacks. Storage: 80-TB usable HDD capacity for Amazon S3 compatible object storage or Amazon EBS compatible block volumes and 28 TB of usable NVMe SSD capacity for Amazon EBS compatible block volumes. Compute: 104 vCPUs, 416 GiB of memory, and an optional NVIDIA Tesla V100 GPU. Devices run Amazon EC2 sbe-c and sbe-g instances, which are equivalent to C5, M5a, G3, and P3 instances.

AWS snowmobile: is an exabyte-scale data transfer service used to move large amounts of data to AWS. You can transfer up to 100 petabytes of data per Snowmobile, a 45-foot long ruggedized shipping container, pulled by a semi trailer truck.


Innovation with AWS
-----------------------------------------------

amazon serverless - applications that dont require provisioning and maintenance of servers such as aws lambda is an example of running serverless applications

artificial intelligence - convert speech to text with amazon transcribe, Discover patterns in text with Amazon Comprehend, Identify potentially fraudulent online activities with Amazon Fraud Detector, Build voice and text chatbots with Amazon Lex amazon augmented ai A2I builds workflows that are required for human review of machine learning predictions. Amazon textract allows to extract text and data from documents to make them more usable for your enterprise.

Machine learning - Amazon SageMaker empowers you to build, train, and deploy ML models quickly. deploy custom models with support for  all the popular open source frameworks. You can use ML to analyze data, solve complex problems, and predict outcomes before they happen. Amazon deep racer - machine learning algorythm that utilizes a physical racing environment to provision deep learning

aws ground station - satellite resource service, pay for the satellite time that you need only.

=======================================================================================

AWS well architected framework -  helps you understand how to design and operate reliable, secure, efficient, and cost-effective systems in the AWS Cloud. It provides a way for you to consistently measure your architecture against best practices and design principles and identify areas for improvement. The well architected framework is based on 6 pillars:
operational excellence: run workloads effectively and gain insights into their operations.  is the ability to run and monitor systems to deliver business value and to continually improve supporting processes and procedures. Design principles for operational excellence in the cloud include performing operations as code, annotating documentation, anticipating failure, and frequently making small, reversible changes.

security: is the ability to protect information, systems, and assets while delivering business value through risk assessments and mitigation strategies. When considering the security of your architecture, apply these best practices: Automate security best practices when possible, Apply security at all layers, Protect data in transit and at rest.

reliability: is the ability of a system to do the following: Recover from infrastructure or service disruptions, Dynamically acquire computing resources to meet demand, Mitigate disruptions such as misconfigurations or transient network issues, Reliability includes testing recovery procedures, scaling horizontally to increase aggregate system availability, and automatically recovering from failure.

performance efficiency: The ability of a workload to consistently and correctly perform its intended functions, is the ability to use computing resources efficiently to meet system requirements and to maintain that efficiency as demand changes and technologies evolve. Evaluating the performance efficiency of your architecture includes experimenting more often, using serverless architectures, and designing systems to be able to go global in minutes.

cost optimization: Cost optimization is the ability to run systems to deliver business value at the lowest price point. Cost optimization includes adopting a consumption model, analyzing and attributing expenditure, and using managed services to reduce the cost of ownership.

sustainability: Sustainability is the ability to continually improve sustainability impacts by reducing energy consumption and increasing efficiency across all components of a workload by maximizing the benefits from the provisioned resources and minimizing the total resources required.To facilitate good design for sustainability: Understand your impact, Establish sustainability goals, Maximize utilization, Anticipate and adopt new, more efficient hardware and software offerings, Use managed services, Reduce the downstream impact of your cloud workloads


Advantages of cloud computing
========================================================================================
 

Operating in the AWS Cloud offers many benefits over computing in on-premises or hybrid environments. In this section, you will learn about six advantages of cloud computing:

Trade upfront expense for variable expense: Upfront expenses include data centers, physical servers, and other resources that you would need to invest in before using computing resources. Instead of investing heavily in data centers and servers before you know how you’re going to use them, you can pay only when you consume computing resources.
   
Benefit from massive economies of scale: By using cloud computing, you can achieve a lower variable cost than you can get on your own. Because usage from hundreds of thousands of customers aggregates in the cloud, providers such as AWS can achieve higher economies of scale. Economies of scale translate into lower pay-as-you-go prices.

Stop guessing capacity: With cloud computing, you don’t have to predict how much infrastructure capacity you will need before deploying an application. For example, you can launch Amazon Elastic Compute Cloud (Amazon EC2) instances when needed and pay only for the compute time you use. Instead of paying for resources that are unused or dealing with limited capacity, you can access only the capacity that you need, and scale in or out in response to demand. 

Increase speed and agility: The flexibility of cloud computing makes it easier for you to develop and deploy applications. This flexibility also provides your development teams with more time to experiment and innovate.

Stop spending money running and maintaining data centers: Cloud computing in data centers often requires you to spend more money and time managing infrastructure and servers. A benefit of cloud computing is the ability to focus less on these tasks and more on your applications and customers.

Go global in minutes: The AWS Cloud global footprint enables you to quickly deploy applications to customers around the world, while providing them with low latency.

=======================================================================================
Exam domains: cloud concepts, security compliance , technology, billing and pricing

=================================================================================

General IT department job roles - 

IT solutions architect - develop the services and solutions, design and manage communications, security, networking and storage. this rols will typically move into the AWS cloud architect role

system admin - keeps servers operational, ensure server uptime goals, patch and upgrade operating systems, hardware, hypervisors, system backups. this job is lost with the implementation of AWS. New AWS roles to consider are either security admin or system operations professional

network admin - administer network access points, personnel access security, configurations and VPNs. same rolse in AWS as above.

Desktop admin - installs and maintains applications on desktop and laptops and work with NW admin for network and security configurations.  Same roles in AWS as above

Applications admin - handles web and customer applications, works with system admin to host and maintain applications on servers, also work with NW admin for application access control. Same roles in AWS as above 

database admin - work with system admin on servers that the database is on. work with NW admin for database control. AWS role would be Devops 


Job roles in the cloud -

Cloud architect  - AWS cloud subject matter expert,  builds the cloud architecture blueprint to deliver highly available, cost efficient and scalable cloud environments. supervises deployment in the cloud environment, application atchitecture for all aspets of the cloud. should understand how services are connected and how to integrate them, understands cloudwatch logging, identity access management and cloud security. recommended learning for this role is cloud essentials learning plan and solutions architect learning plan, certifications include AWS Cloud Practitioner Foundational, AWS Solutions Architect Associate, AWS Solutions Architect Professional

System admin - responsible for overall performance of cloud systems, manages configurations and changes, assists with setting up and accessing database servers in the cloud, deploys, configures and monitors cloud solutions, Proficient with hands-on tasks and detailed configuration changes, Understands configuration management, Proficient with requirement gathering and translating to deployments, Recommended Learning Plans are available here: Cloud Essentials Learning Plan, Systems Operator Learning Plan. Additional certifications you can consider:AWS Cloud Practitioner Foundational, AWS Solutions Architect Associate, AWS SysOps Administrator Associate, AWS Advanced Networking Specialty

security admin - responsible for protection of data and resources in the cloud, must hava a reactive mindset to investigate security incidents and a proactive one to plug holes in security. Defines security requirements for enterprise-level businesses based on their security and regulatory requirements, Understands security rules and requirements, Communicates security rules down to engineers and up to decision makers to understand and address security risks, Resourceful because it is not possible to memorize all regulatory requirements, Researches and is resourceful with contacts to determine an answer to address security. Recommended Learning Plans are available here: Cloud Essentials Learning Plan, Security Learning Plan. Additional certifications you can consider: AWS Solutions Architect Associate, AWS Security Administration

devops admin - oversees database and developer teams, optmizes use of aws cloud, helps cloud operations run at larger faster scale. Applies programming scripting languages with proficiency, Understands and applies QA and testing, Understands operations and manages developers, Orchestrates the many tools and stages in the pipeline with a model of small rapid releases that can each be tested and improved ongoing, manages release cycle into the pipeline, rollback changes incase something doesnt work, Recommended Learning Plans are available here: Cloud Essentials Learning Plan, DevOps Engineer Learning Plan. Additional certifications you can consider: AWS Cloud Practitioner Foundational, AWS Solutions Architect Associate, AWS Developer Associate, AWS DevOps Engineer Professional, AWS Database Specialty

===============================================================================================================================

AWS billing and cost management

you can reach these tools in the AWS billing dashboard by typing "billing" in the search

tools to establish visibility:

AWS cost explorer - visualise, understand and manage your aws costs over time. where you can view costs up to 12 months in the past and filter by service, for example you can isolate charges by AWS region or service, there is a cost explorer api. Used to visualize charges charts directly in the AWS management console.
 
AWS cost and usage reports - achieve visibility into your aws spend, track your usage and provide estimated charges for the account based around individual aws products, these reports can be further analyzed using amazon Athena and Amazon quicksight

tools to prevent overspend:

AWS budgets - improve planning and cost control with flexible budgeting and forecasting. A cost management feature used to track and manage aws costs. create a budget that stops you from over spending. track costs based on filters for AWS services, member accounts, regions, tags etc. create alerts for your budget that send emails or restrict your account usage, geneerate reports on a daily basis or any timeline, there is an option for a zero spend budget haha. After creating a budget the cost exploreer graph will appear aftewr 24 hours, updates 3 times a day

AWS cost anomaly detection - reduce cost surprises with machine learning, found in billing and payments

Billing console: resources to manage ongoin payments and generate reports 
Cost management console: resources to optimize future costs

unblended costs: Unblended costs represent your usage costs on the day they are charged to you. In finance terms, they represent your costs on a cash basis of accounting. default option for showing costs

Amortized costs:able to show daily cost breakdowns of everything, even your monthly charges which would be broken down on a dailyh basis it’s useful to view costs on an accrual basis rather than a cash basis. This cost dataset is especially useful for those of you who have purchased AWS Reservations such as Amazon EC2 Reserved Instances. Savings Plans and Reservations often have upfront or recurring monthly fees associated with them.

blended costs: Blended costs are calculated by multiplying each account’s service usage against something called a blended rate. A blended rate is the average rate of on-demand usage, as well as Savings Plans- and reservation-related usage, that is consumed by member accounts in an organization for a particular service.

===============================================================================================
Compute as a service 

there are 3 available compute options: Virtual machines, container services and serverless, more info on these topics in above sections 

container services include: Amazon Elastic compute service and Elastic kubernetes service 
serverless include:  Lambda and Fargate
Virtul machines incluse: EC2

Virtual machines: easiest to setup, emulates a physical server and allows install of a Http server to run applications. Hypervisor is needed to install a vm on AWS, which controls hardware resources for your vm. Amazon elastic cloud EC2 aids in setup of hypervisor and host machines during setup and installs the VM operating system

Servers: the first building block you need to host an application. Srevers usually handle HTTP requests and send responses following the client server model. ALthough any API based communication also falls under this model. Common HTTP servers include:
Windows options, suc as internet information sewrvice IIS and Linux options such as Apache HTTP server, Nginx, and Apache Tomcat.

=================================================================================================Availability

The availability of a system is typically expressed as a percentage of uptime in a given year or as a number of nines. In the following table is a list of availability percentages based on the downtime per year and its notation in nines. for example 90% is one 9 of availability and a downtie of 36.53 days or 99.99% availbility is four 9's of availability and 52.60 minutes of downtime and so on.

To increase availability, you need redundancy. This typically means more infrastructure—more data centers, more servers, more databases, and more replication of data. You can imagine that adding more of this infrastructure means a higher cost. Customers want the application to always be available, but you need to draw a line where adding redundancy is no longer viable in terms of revenue.

the way to do this is to add instances across availability zones or to add caching and storage across availability zones, there is a section on how to do this above. However, when there is more than one instance, it brings new challenges, such as the following:

Replication process – The first challenge with multiple EC2 instances is that you need to create a process to replicate the configuration files, software patches, and application across instances. The best method is to automate where you can.

Customer redirection – The second challenge is how to notify the clients—the computers sending requests to your server—about the different servers. You can use various tools here. The most common is using a Domain Name System (DNS) where the client uses one record that points to the IP address of all available servers.
However, this method isn't always used because of propagation — the time frame it takes for DNS changes to be updated across the Internet.

Another option is to use a load balancer, which takes care of health checks and distributing the load across each server. Situated between the client and the server, a load balancer avoids propagation time issues. You will learn more about load balancers in the next section.

Types of high availability – The last challenge to address when there is more than one server is the type of availability you need: active-passive or active-active. With an active-passive system, only one of the two instances is available at a time. One advantage of this method is that for stateful applications (where data about the client’s session is stored on the server), there won’t be any issues. This is because the customers are always sent to the server where their session is stored. A disadvantage of an active-passive system is scalability. This is where an active-active system shines. With both servers available, the second server can take some load for the application, and the entire system can take more load. However, if the application is stateful, there would be an issue if the customer’s session isn’t available on both servers. Stateless applications work better for active-active systems.

================================================================================================

amazon Q developer - pay to have your code stolen from you and develop your replacement
